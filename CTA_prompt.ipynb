{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取 CSV 文件\n",
    "file_path = '/home/yongkang/work/TableLlama/generate_prompt/test_semi_sotab-re-withunlabel.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data['label'] = data['label'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_id</th>\n",
       "      <th>column_id</th>\n",
       "      <th>label</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Product_beautyinstonejewelry.com_September2020...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-10-07; 2020-10-07; 2020-10-09; 2020-10-03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product_beautyinstonejewelry.com_September2020...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>USD; USD; USD; USD; USD; USD; USD; USD; USD; U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Product_beautyinstonejewelry.com_September2020...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Beauty In Stone Jewelry; Beauty In Stone Jewel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Product_beautyinstonejewelry.com_September2020...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>40\\\" Chain Classic designer inspired famous fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Product_beautyinstonejewelry.com_September2020...</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "      <td>69; 49; 99; 65; 7; 30; 20; 59; 69; 80; 69; 99;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            table_id  column_id  label  \\\n",
       "0  Product_beautyinstonejewelry.com_September2020...          6      0   \n",
       "1  Product_beautyinstonejewelry.com_September2020...          2      1   \n",
       "2  Product_beautyinstonejewelry.com_September2020...          5      2   \n",
       "3  Product_beautyinstonejewelry.com_September2020...          1     -1   \n",
       "4  Product_beautyinstonejewelry.com_September2020...          3     -1   \n",
       "\n",
       "                                                data  \n",
       "0  2020-10-07; 2020-10-07; 2020-10-09; 2020-10-03...  \n",
       "1  USD; USD; USD; USD; USD; USD; USD; USD; USD; U...  \n",
       "2  Beauty In Stone Jewelry; Beauty In Stone Jewel...  \n",
       "3  40\\\" Chain Classic designer inspired famous fl...  \n",
       "4  69; 49; 99; 65; 7; 30; 20; 59; 69; 80; 69; 99;...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from time import time\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# import pytrec_eval\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "import wandb\n",
    "\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "import transformers\n",
    "from torch.utils import data\n",
    "from torch.nn.utils import rnn\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "from itertools import chain\n",
    "import copy\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yongkang/miniconda3/envs/observatory/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from supervised_fine_tune import PROMPT_DICT\n",
    "def generate_prompt(instruction, question, input_seg=None):\n",
    "  if input:\n",
    "    return PROMPT_DICT[\"prompt_input\"].format(instruction=instruction, input_seg=input_seg, question=question)\n",
    "  else:\n",
    "    return PROMPT_DICT[\"prompt_no_input\"].format(instruction=instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import math\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    \"osunlp/TableLlama\",\n",
    "    cache_dir=\"./cache\",\n",
    ")\n",
    "\n",
    "orig_ctx_len = getattr(config, \"max_position_embeddings\", None)\n",
    "if orig_ctx_len and 8192 > orig_ctx_len:\n",
    "    scaling_factor = float(math.ceil(8192 / orig_ctx_len))\n",
    "    config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    \"osunlp/TableLlama\",\n",
    "    cache_dir=\"./cache\",\n",
    "    model_max_length=8192 if 8192 > orig_ctx_len else orig_ctx_len,\n",
    "    # padding_side=\"right\",\n",
    "    padding_side=\"left\",\n",
    "    use_fast=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TURL-CTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the type vocab\n",
    "with open(\"/home/yongkang/work/TableLlama/generate_prompt/type_vocab_turl-cta.txt\", \"r\") as file:\n",
    "    id2type = {int(line.split(\"\\t\")[0]): line.split(\"\\t\")[1].strip() for line in file}\n",
    "type2id = {v: k for k, v in id2type.items()}\n",
    "types = list(type2id.keys())\n",
    "\n",
    "all_label_names = \", \".join(id2type.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     table_id                            pgTitle              secTitle  \\\n",
      "0   1001122-3                    elon university            greek life   \n",
      "1   1001122-3                    elon university            greek life   \n",
      "2   1001122-3                    elon university            greek life   \n",
      "3  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "4  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "5  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "6  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "7  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "8  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "9  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "\n",
      "  caption                        headers  \\\n",
      "0     NaN        interfraternity council   \n",
      "1     NaN  national pan-hellenic council   \n",
      "2     NaN            panhellenic council   \n",
      "3     NaN                           seat   \n",
      "4     NaN                       pre-1943   \n",
      "5     NaN                     pre-1943.1   \n",
      "6     NaN                     pre-1943.2   \n",
      "7     NaN                     pre-1943.3   \n",
      "8     NaN                          swing   \n",
      "9     NaN                      post-1943   \n",
      "\n",
      "                                               label  column_index  \\\n",
      "0  [organization.organization, education.fraterni...             0   \n",
      "1  [organization.organization, education.fraterni...             1   \n",
      "2  [organization.organization, education.fraterni...             2   \n",
      "3                                [location.location]             0   \n",
      "4                                                 []             1   \n",
      "5                                                 []             2   \n",
      "6             [people.person, government.politician]             3   \n",
      "7                                                 []             4   \n",
      "8                                                 []             5   \n",
      "9                                                 []             6   \n",
      "\n",
      "                                                data  \n",
      "0             ΚΑ; ΚΣ; ΛΧΑ; ΠΚΦ; ΣΧ; ΣΦΕ; ΣΠ; ΔΥ; ZBT  \n",
      "1             ΑΦΑ; ΚΑΨ; ΩΨΦ; ΦΒΣ; ΑΚΑ; ΔΣΘ; ΖΦΒ; ΣΓΡ  \n",
      "2           ΑΧΩ; ΑΟΠ; ΑΞΔ; ΔΔΔ; ΚΔ; ΦΜ; ΣΚ; ΣΣΣ; ZTA  \n",
      "3  Adelaide, SA; Barker, SA; Boothby, SA; Denison...  \n",
      "4  Party; nan; nan; nan; nan; nan; nan; nan; nan;...  \n",
      "5  Member; United Australia; Country; United Aust...  \n",
      "6  Fred Stacey; Archie Cameron *; Grenfell Price;...  \n",
      "7  Margin; 4.7; nan; 6.6; 1.1; 4.8; 7.7; 0.9; 9.6...  \n",
      "8  Member; 20.3; 14.2; 16.1; 10.1; 10.8; 10.2; 7....  \n",
      "9  Party; 15.6; 1.7; 0.9; 9.0; 5.4; 2.5; 6.3; 0.4...  \n"
     ]
    }
   ],
   "source": [
    "# read df\n",
    "import os\n",
    "import pandas as pd\n",
    "filepath = \"/home/yongkang/work/TableLlama/generate_prompt/turl_cta_test_all_with_metadata.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "df['label'] = df['label'].apply(lambda x: eval(x))\n",
    "index_list = df['label'].apply(lambda x: [i for i, value in enumerate(x) if value == 1])\n",
    "\n",
    "label_names = []\n",
    "for indices in index_list:\n",
    "    labels = [id2type.get(idx) for idx in indices]\n",
    "    label_names.append(labels)\n",
    "\n",
    "# 将结果保存到 DataFrame 中（可选）\n",
    "df['label'] = label_names\n",
    "\n",
    "# 输出结果\n",
    "print(df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4764 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4764/4764 [00:11<00:00, 402.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import time, json\n",
    "\n",
    "num_row = 3 # how many rows to include in the table prompt\n",
    "k = 5\n",
    "output_data = []\n",
    "\n",
    "for tqdm_idx, group_df in tqdm(df.groupby('table_id')):\n",
    "    group_df.sort_values(by=['column_index'], inplace=True)\n",
    "    group_df.reset_index(drop=True, inplace=True)\n",
    "    group_df['data'] = group_df['data'].astype(str)\n",
    "    \n",
    "    pgTitle = group_df['pgTitle'].iloc[0]\n",
    "    secTitle = group_df['secTitle'].iloc[0]\n",
    "    caption = group_df['caption'].iloc[0]\n",
    "    wiki_info = f\"[TLE] \"\n",
    "    if pgTitle != \"nan\":\n",
    "        wiki_info += f\"The Wikipedia page is about {pgTitle}. \"\n",
    "    if secTitle != \"nan\":\n",
    "        wiki_info += f\"The Wikipedia section is about {secTitle}. \"\n",
    "    if caption != \"nan\":\n",
    "        wiki_info += f\"The table caption is about {caption}. \"\n",
    "\n",
    "    col_values = []\n",
    "    min_row_len = min([len(row['data'].split(\";\")) for row_idx, row in group_df.iterrows()])\n",
    "    min_row_len = min(min_row_len, num_row)\n",
    "    for column_index, col in group_df.iterrows():\n",
    "        col_values.append(col['data'].split(\";\")[:min_row_len])\n",
    "    # transpose the list\n",
    "    col_values = list(map(list, zip(*col_values)))\n",
    "\n",
    "\n",
    "    col_head = f\"[TAB] col: | \"\n",
    "    for column_index in range(1, len(group_df) + 1):\n",
    "        col_head += f\"{group_df.iloc[column_index-1]['headers']} |\"\n",
    "\n",
    "    table_cell = \"\"\n",
    "    for row_idx in range(1, min_row_len + 1):\n",
    "        example_values = \" | \".join(col_values[row_idx-1])  # Extract first 5 \n",
    "        if row_idx == 1:\n",
    "            table_cell += f\"row {row_idx}: | {example_values} |\"\n",
    "        else:\n",
    "            table_cell += f\" [SEP] row {row_idx}: | {example_values} |\"\n",
    "\n",
    "    df_target = group_df[group_df['label'].apply(lambda x: len(x) != 0)]\n",
    "    for idx in range(len(df_target)):\n",
    "        column_index = df_target.iloc[idx][\"column_index\"]+1\n",
    "        label = df_target.iloc[idx][\"label\"]\n",
    "        header = df_target.iloc[idx][\"headers\"]\n",
    "        entities = \", \".join([f\"<{item.strip()}>\" for item in df_target.iloc[idx][\"data\"].split(\";\")[:k]])\n",
    "\n",
    "        table_prompt = {}\n",
    "        table_prompt[\"table_id\"] = str(tqdm_idx)\n",
    "        table_prompt[\"instruction\"] = \"This is a column type annotation task. The goal for this task is to choose correct types for one selected column of the table from the given candidates. The Wikipedia page, section and table caption (if any) provide important information for choosing the correct column types.\"\n",
    "        table_prompt[\"input\"] = wiki_info + col_head + table_cell\n",
    "        table_prompt[\"question\"] = f\"The column '{header}' contains following entities: {entities}, stc. The column type candidates are: {all_label_names}. What are the correct column types for this column (column name: {header}; entities: {entities}, etc)?\"\n",
    "        table_prompt[\"ground_truth\"] = label\n",
    "        table_prompt[\"entity\"] = [item.strip() for item in df_target.iloc[idx][\"data\"].split(\";\")[:k]]\n",
    "        table_prompt[\"output\"] = \", \".join(label)\n",
    "        table_prompt[\"input_seg\"] = wiki_info + col_head + table_cell\n",
    "        output_data.append(table_prompt)\n",
    "    \n",
    "        \n",
    "output_file = './dataset/turl-cta.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TURL-CPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'baseball.baseball_player.batting_stats-baseball.batting_statistics.team, book.author.works_written, time.event.locations, film.film.genre, government.politician.party-government.political_party_tenure.party, people.person.place_of_birth, film.film.release_date_s-film.film_regional_release_date.film_release_region, american_football.football_player.position_s, cvg.computer_videogame.publisher, people.person.spouse_s-people.marriage.spouse, film.actor.film-film.performance.character, sports.sports_team.sport, base.biblioness.bibs_location.country, sports.sports_facility.home_venue_for-sports.team_venue_relationship.team, award.award_nominated_work.award_nominations-award.award_nomination.ceremony, film.film.directed_by, base.culturalevent.event.entity_involved, music.album.artist, sports.sports_team.location, sports.sports_position.players-sports.sports_team_roster.position, sports.pro_athlete.teams-sports.sports_team_roster.position, soccer.football_player.statistics-soccer.football_player_stats.team, basketball.basketball_player.player_statistics-basketball.basketball_player_stats.team, location.hud_county_place.county, sports.sports_team.league-sports.sports_league_participation.league, aviation.airline.hubs, organization.organization.headquarters-location.mailing_address.state_province_region, sports.sports_championship_event.champion, location.location.nearby_airports, people.person.parents, film.film.distributors-film.film_film_distributor_relationship.distributor, sports.sports_facility.teams, film.film.written_by, base.wikipedia_infobox.video_game.developer, film.film.produced_by, award.award_winning_work.awards_won-award.award_honor.award_winner, baseball.baseball_team.league, award.award_nominated_work.award_nominations-award.award_nomination.award_nominee, education.educational_institution.athletics_brand, olympics.olympic_participating_country.athletes-olympics.olympic_athlete_affiliation.athlete, location.administrative_division.capital-location.administrative_division_capital_relationship.capital, aviation.airport.serves, award.award_ceremony.nominees-award.award_nomination.award_nominee, location.location.containedby, tv.tv_actor.starring_roles-tv.regular_tv_appearance.character, location.country.official_language, tv.tv_character.appeared_in_tv_program-tv.regular_tv_appearance.actor, base.schemastaging.athlete_extra.salary-base.schemastaging.athlete_salary.team, sports.sports_award_winner.awards-sports.sports_award.team, olympics.olympic_event_competition.medalists-olympics.olympic_medal_honor.medalist, sports.sports_team.league-sports.sports_league_participation.from, location.location.street_address-location.mailing_address.citytown, soccer.football_player.position_s, location.location.contains, music.composition.composer, ice_hockey.hockey_player.hockey_position, government.political_party.politicians_in_this_party-government.political_party_tenure.politician, base.wikipedia_infobox.video_game.platforms, government.governmental_jurisdiction.governing_officials-government.government_position_held.office_holder, government.politician.government_positions_held-government.government_position_held.district_represented, film.film.language, military.military_person.service-military.military_service.military_force, award.award_category.winners-award.award_honor.award_winner, tv.tv_network.programs-tv.tv_network_duration.program, soccer.football_team.player_statistics-soccer.football_player_stats.player, film.director.film, organization.organization.headquarters-location.mailing_address.country, sports.sports_league.teams-sports.sports_league_participation.team, award.award_winner.awards_won-award.award_honor.honored_for, location.country.capital, tv.tv_program.country_of_origin, film.film_character.portrayed_in_films-film.performance.actor, location.country.languages_spoken, film.film.country, base.schemastaging.organization_extra.phone_number-base.schemastaging.phone_sandbox.service_location, time.event.instance_of_recurring_event, government.political_district.representatives-government.government_position_held.office_holder, cvg.computer_videogame.developer, military.military_combatant.military_conflicts-military.military_combatant_group.combatants, olympics.olympic_athlete.medals_won-olympics.olympic_medal_honor.country, base.aareas.schema.administrative_area.administrative_parent, music.composer.compositions, award.award_nominee.award_nominations-award.award_nomination.nominated_for, film.film.starring-film.performance.character, olympics.olympic_athlete.medals_won-olympics.olympic_medal_honor.event, olympics.olympic_participating_country.medals_won-olympics.olympic_medal_honor.medalist, people.person.places_lived-people.place_lived.location, sports.pro_athlete.teams-sports.sports_team_roster.team, film.film.music, people.person.nationality, tv.tv_program.original_network-tv.tv_network_duration.network, award.award_ceremony.nominees-award.award_nomination.nominated_for, film.actor.film-film.performance.film, people.deceased_person.place_of_death, sports.sports_team.roster-sports.sports_team_roster.player, sports.sports_position.players-sports.sports_team_roster.player, book.written_work.author, location.administrative_division.country, olympics.olympic_athlete.country-olympics.olympic_athlete_affiliation.country, sports.sports_league_season.league, sports.sports_team.venue-sports.team_venue_relationship.venue, meteorology.tropical_cyclone.tropical_cyclone_season, award.award_category.category_of, film.film.story_by, broadcast.broadcast.area_served, film.film.production_companies, award.award_category.nominees-award.award_nomination.ceremony, music.artist.album, award.award_category.nominees-award.award_nomination.award_nominee, film.film.starring-film.performance.actor, organization.organization.headquarters-location.mailing_address.citytown, tv.tv_actor.starring_roles-tv.regular_tv_appearance.series, baseball.baseball_player.position_s, award.award_category.winners-award.award_honor.ceremony, people.person.education-education.education.institution, tv.tv_program.regular_cast-tv.regular_tv_appearance.actor, award.award_ceremony.awards_presented-award.award_honor.award_winner, american_football.football_player.games-american_football.player_game_statistics.team, location.capital_of_administrative_division.capital_of-location.administrative_division_capital_relationship.administrative_division, sports.sports_team.arena_stadium, award.award_ceremony.awards_presented-award.award_honor.honored_for'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the type vocab\n",
    "with open(\"./generate_prompt/type_vocab_turl-cpa.txt\", \"r\") as file:\n",
    "    id2type = {int(line.split(\"\\t\")[0]): line.split(\"\\t\")[1].strip() for line in file}\n",
    "type2id = {v: k for k, v in id2type.items()}\n",
    "types = list(type2id.keys())\n",
    "\n",
    "all_label_names = \", \".join(id2type.values())\n",
    "all_label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     table_id                            pgTitle              secTitle  \\\n",
      "0  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "1  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "2  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "3  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "4  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "5  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "6  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "7  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "8  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "9  10039383-3  australian federal election, 1943  seats changing hands   \n",
      "\n",
      "  caption      headers                                              label  \\\n",
      "0     NaN         seat                                                 []   \n",
      "1     NaN     pre-1943                                                 []   \n",
      "2     NaN   pre-1943.1                                                 []   \n",
      "3     NaN   pre-1943.2  [government.political_district.representatives...   \n",
      "4     NaN   pre-1943.3                                                 []   \n",
      "5     NaN        swing                                                 []   \n",
      "6     NaN    post-1943                                                 []   \n",
      "7     NaN  post-1943.1  [government.political_district.representatives...   \n",
      "8     NaN  post-1943.2                                                 []   \n",
      "9     NaN  post-1943.3                                                 []   \n",
      "\n",
      "   column_index                                               data  \\\n",
      "0             0  Adelaide, SA; Barker, SA; Boothby, SA; Denison...   \n",
      "1             1  Party; nan; nan; nan; nan; nan; nan; nan; nan;...   \n",
      "2             2  Member; United Australia; Country; United Aust...   \n",
      "3             3  Fred Stacey; Archie Cameron *; Grenfell Price;...   \n",
      "4             4  Margin; 4.7; nan; 6.6; 1.1; 4.8; 7.7; 0.9; 9.6...   \n",
      "5             5  Member; 20.3; 14.2; 16.1; 10.1; 10.8; 10.2; 7....   \n",
      "6             6  Party; 15.6; 1.7; 0.9; 9.0; 5.4; 2.5; 6.3; 0.4...   \n",
      "7             7  Cyril Chambers; Archie Cameron; Thomas Sheehy;...   \n",
      "8             8  nan; Labor; United Australia; Labor; Labor; La...   \n",
      "9             9  nan; nan; nan; nan; nan; nan; nan; nan; nan; n...   \n",
      "\n",
      "          mark  \n",
      "0      subject  \n",
      "1  non-subject  \n",
      "2  non-subject  \n",
      "3  non-subject  \n",
      "4  non-subject  \n",
      "5  non-subject  \n",
      "6  non-subject  \n",
      "7  non-subject  \n",
      "8  non-subject  \n",
      "9  non-subject  \n"
     ]
    }
   ],
   "source": [
    "# read df\n",
    "import os\n",
    "import pandas as pd\n",
    "filepath = \"/home/yongkang/work/TableLlama/generate_prompt/turl_cpa_test_all_with_metadata.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "df['label'] = df['label'].apply(lambda x: eval(x))\n",
    "index_list = df['label'].apply(lambda x: [i for i, value in enumerate(x) if value == 1])\n",
    "\n",
    "label_names = []\n",
    "for indices in index_list:\n",
    "    labels = [id2type.get(idx) for idx in indices]\n",
    "    label_names.append(labels)\n",
    "\n",
    "# 将结果保存到 DataFrame 中（可选）\n",
    "df['label'] = label_names\n",
    "\n",
    "# 输出结果\n",
    "print(df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1467 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1467/1467 [00:07<00:00, 190.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "num_row = 3 # how many rows to include in the table prompt\n",
    "k = 5\n",
    "output_data = []\n",
    "\n",
    "for tqdm_idx, group_df in tqdm(df.groupby('table_id')):\n",
    "    group_df['column_index'] = group_df['column_index'].astype(int)\n",
    "    group_df.sort_values(by=['column_index'], inplace=True)\n",
    "    group_df.reset_index(drop=True, inplace=True)\n",
    "    group_df['data'] = group_df['data'].astype(str)\n",
    "\n",
    "    col_values = []\n",
    "    min_row = min([len(row['data'].split(\";\")) for row_idx, row in group_df.iterrows()])\n",
    "    min_row_len = min(min_row, num_row)\n",
    "    for column_index, col in group_df.iterrows():\n",
    "        col_values.append(col['data'].split(\";\")[:min_row_len])\n",
    "    # transpose the list\n",
    "    col_values = list(map(list, zip(*col_values)))\n",
    "\n",
    "\n",
    "    col_head = f\"[TAB] col: | \"\n",
    "    for column_index in range(1, len(group_df) + 1):\n",
    "        col_head += f\"{group_df.iloc[column_index-1]['headers']} |\"\n",
    "\n",
    "    table_cell = \"\"\n",
    "    for row_idx in range(1, min_row_len + 1):\n",
    "        example_values = \" | \".join(col_values[row_idx-1])  # Extract first 5 \n",
    "        if row_idx == 1:\n",
    "            table_cell += f\"row {row_idx}: | {example_values} |\"\n",
    "        else:\n",
    "            table_cell += f\" [SEP] row {row_idx}: | {example_values} |\"\n",
    "\n",
    "    df_target = group_df[group_df['label'].apply(lambda x: len(x) != 0)]\n",
    "    for idx in range(len(df_target)):\n",
    "        column_index = df_target.iloc[idx][\"column_index\"]+1\n",
    "        label = df_target.iloc[idx][\"label\"]\n",
    "        header = df_target.iloc[idx][\"headers\"]\n",
    "        \n",
    "        k = min(5, min_row)\n",
    "        entity_pairs = \", \".join(\n",
    "            f\"<({group_df.iloc[0]['data'].split(';')[i]}), ({df_target.iloc[idx]['data'].split(';')[i]})>\"\n",
    "            for i in range(k)\n",
    "        )\n",
    "\n",
    "        table_prompt = {}\n",
    "        table_prompt[\"table_id\"] = str(tqdm_idx)\n",
    "        table_prompt[\"instruction\"] = \"This is a relation extraction task. The goal for this task is to choose correct relations between two selected columns of the table from the given candidates. The Wikipedia page, section and table caption (if any) provide important information for choosing the correct relation types.\"\n",
    "        table_prompt[\"question\"] = f\"The two selected column names are: <({group_df.iloc[0]['headers']}),({header})>. The entity pairs for these two columns are: {entity_pairs}, etc. The relation type candidates are: {all_label_names}. What are the correct relation type for the two selected columns (column name: <({group_df.iloc[0]['headers']}),({header})> entity pairs: {entity_pairs}, etc)?\"\n",
    "        table_prompt[\"input\"] = col_head + table_cell\n",
    "        table_prompt[\"ground_truth\"] = label\n",
    "        table_prompt[\"output\"] = \", \".join(label)\n",
    "        table_prompt[\"input_seg\"] = col_head + table_cell\n",
    "        output_data.append(table_prompt)\n",
    "    \n",
    "        \n",
    "output_file = './dataset/turl-cpa.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOTAB-CTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the type vocab\n",
    "with open(\"/home/yongkang/work/TableLlama/generate_prompt/type_vocab_sotab-cta.txt\", \"r\") as file:\n",
    "    id2type = {int(line.split(\"\\t\")[0]): line.split(\"\\t\")[1].strip() for line in file}\n",
    "type2id = {v: k for k, v in id2type.items()}\n",
    "types = list(type2id.keys())\n",
    "\n",
    "all_label_names = \", \".join(id2type.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            table_id  column_index  \\\n",
      "0  Product_beautyinstonejewelry.com_September2020...             3   \n",
      "1  Product_beautyinstonejewelry.com_September2020...             2   \n",
      "2  Product_beautyinstonejewelry.com_September2020...             6   \n",
      "3  Product_beautyinstonejewelry.com_September2020...             5   \n",
      "4  Product_beautyinstonejewelry.com_September2020...             4   \n",
      "5  Product_beautyinstonejewelry.com_September2020...             0   \n",
      "6  Product_beautyinstonejewelry.com_September2020...             1   \n",
      "7  Product_royalpoolandspa.com_September2020_CTA....             0   \n",
      "8  Product_royalpoolandspa.com_September2020_CTA....             1   \n",
      "9  Product_royalpoolandspa.com_September2020_CTA....             2   \n",
      "\n",
      "              label                                               data  \n",
      "0             price  69; 49; 99; 65; 7; 30; 20; 59; 69; 80; 69; 99;...  \n",
      "1          currency  USD; USD; USD; USD; USD; USD; USD; USD; USD; U...  \n",
      "2              Date  2020-10-07; 2020-10-07; 2020-10-09; 2020-10-03...  \n",
      "3             Brand  Beauty In Stone Jewelry; Beauty In Stone Jewel...  \n",
      "4  ItemAvailability  https://schema.org/InStock; https://schema.org...  \n",
      "5                -1  40\\\" Chain Classic designer inspired famous fl...  \n",
      "6                -1  Designer Inspired Chain Necklace Glass Dome Pe...  \n",
      "7              Text  Test for: Free Chlorine/Bromine, pH, and Alkal...  \n",
      "8      Product/name  AquaChek TruTest Strips; AquaChek TruTest Digi...  \n",
      "9             price  $7.90; $58.00; $17.50; $6.50; $165.00; $13.00;...  \n"
     ]
    }
   ],
   "source": [
    "# read df\n",
    "import os\n",
    "import pandas as pd\n",
    "filepath = \"./generate_prompt/test_semi_sotab_withunlabel.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "df['label'] = df['label'].map(id2type).fillna(-1)\n",
    "print(df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7026/7026 [00:15<00:00, 453.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "num_row = 3 # how many rows to include in the table prompt\n",
    "k = 5\n",
    "output_data = []\n",
    "\n",
    "for tqdm_idx, group_df in tqdm(df.groupby('table_id')):\n",
    "    group_df.sort_values(by=['column_index'], inplace=True)\n",
    "    group_df.reset_index(drop=True, inplace=True)\n",
    "    group_df['data'] = group_df['data'].astype(str)\n",
    "\n",
    "    col_values = []\n",
    "    min_row_len = min([len(row['data'].split(\";\")) for row_idx, row in group_df.iterrows()])\n",
    "    min_row_len = min(min_row_len, num_row)\n",
    "    for column_index, col in group_df.iterrows():\n",
    "        col_values.append(col['data'].split(\";\")[:min_row_len])\n",
    "    # transpose the list\n",
    "    col_values = list(map(list, zip(*col_values)))\n",
    "\n",
    "    col_head = f\"[TAB] col: | \"\n",
    "    for column_index in range(1, len(group_df) + 1):\n",
    "        col_head += f\"Column {column_index} | \"\n",
    "\n",
    "    table_cell = \"\"\n",
    "    for row_idx in range(1, min_row_len + 1):\n",
    "        example_values = \" | \".join(col_values[row_idx-1])  # Extract first 5 \n",
    "        if row_idx == 1:\n",
    "            table_cell += f\"row {row_idx}: | {example_values} |\"\n",
    "        else:\n",
    "            table_cell += f\" [SEP] row {row_idx}: | {example_values} |\"\n",
    "\n",
    "    df_target = group_df[group_df[\"label\"] != -1]\n",
    "    for idx in range(len(df_target)):\n",
    "        column_index = df_target.iloc[idx][\"column_index\"]+1\n",
    "        label = df_target.iloc[idx][\"label\"]\n",
    "        entities = \", \".join([f\"<{item.strip()}>\" for item in df_target.iloc[idx][\"data\"].split(\";\")[:k]])\n",
    "\n",
    "        table_prompt = {}\n",
    "        table_prompt[\"table_id\"] = str(tqdm_idx)\n",
    "        table_prompt[\"instruction\"] = \"This is a column type annotation task. The goal for this task is to choose the correct type for one selected column of the table from the given candidates.\"\n",
    "        table_prompt[\"input\"] = col_head + table_cell\n",
    "        table_prompt[\"question\"] = f\"The column 'Column {column_index}' contains following entities: {entities}, etc. The column type candidates are: {all_label_names}. What are the correct column type for this column (column name: Column {column_index}; entities: {entities}, etc)?\"\n",
    "        table_prompt[\"ground_truth\"] = [label]\n",
    "        table_prompt[\"entity\"] = [item.strip() for item in df_target.iloc[idx][\"data\"].split(\";\")[:k]]\n",
    "        table_prompt[\"output\"] = label\n",
    "        table_prompt[\"input_seg\"] = col_head + table_cell\n",
    "        output_data.append(table_prompt)\n",
    "    \n",
    "        \n",
    "output_file = './dataset/sotab-cta.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOTAB-CPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'priceValidUntil, priceCurrency, brand, productID, gtin13, sku, postalCode, telephone, itemCondition, availability, mpn, endDate, startDate, worstRating, addressLocality, addressRegion, reviewCount, bestRating, ratingValue, openingHours, longitude, latitude, description, performer, category, price, validFrom, url, image, weight, servingSize, fatContent, carboHydrateContent, proteinContent, sugarContent, saturatedFatContent, datePublished, prepTime, recipeYield, recipeIngredient, cookTime, recipeCategory, totalTime, keywords, recipeCuisine, recipeInstructions, calories, sodiumContent, releaseDate, manufacturer, color, model, organizer, addressCountry, workLocation, gtin8, location, eventStatus, review, email, transFatContent, cholesterolContent, fiberContent, birthDate, deathDate, inventoryLevel, opens, dayOfWeek, closes, contactType, priceRange, legalName, address, author, duration, byArtist, inAlbum, ratingCount, unsaturatedFatContent, gtin12, streetAddress, publisher, headline, text, dateModified, eventAttendanceMode, availableDeliveryMethod, itemReviewed, about, width, depth, height, unitCode, amenityFeature, areaServed, genre, director, disambiguatingDescription, memberOf, availabilityStarts, isbn, competitor, currenciesAccepted, faxNumber, gender, gtin14, contentRating, performTime, cookingMethod, founder, identifier, inLanguage, bookFormat, paymentAccepted, unitText, award, productionDate, gtin, partOfSeries, awayTeam, homeTeam, honorificSuffix, worksFor, jobTitle, dateCreated, productionCompany, offers, sameAs, checkoutTime, checkInTime, acceptedPaymentmethod, suitableForDiet, numberOfPages, actor, datePosted, title, employmentType, typicalAgeRange, affiliation, familyName, givenName, validThrough, photo, serialNumber, track, deliveryLeadTime, material, alumniOf, birthPlace, logo, episodeNumber, employee, audienceType, servesCuisine, homeLocation, hasMap, nationality, doorTime, availableLanguage, isAccessibleForFree, makesOffer, creator, copyrightHolder, copyrightYear, alternativeHeadline, additionalName, hiringOrganization, countryOfOrigin, alternateName, eligibleQuantity, bookEdition, knowsLanguage, userInteractionCount, interactionType, numTracks, warranty'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the type vocab\n",
    "with open(\"./generate_prompt/type_vocab_sotab-cpa.txt\", \"r\") as file:\n",
    "    id2type = {int(line.split(\"\\t\")[0]): line.split(\"\\t\")[1].strip() for line in file}\n",
    "type2id = {v: k for k, v in id2type.items()}\n",
    "\n",
    "print(len(id2type))\n",
    "all_label_names = \", \".join(id2type.values())\n",
    "all_label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           table_id  column_id          label  \\\n",
      "2783   Book_2d-market.com_September2020_CPA.json.gz          8          price   \n",
      "2784   Book_2d-market.com_September2020_CPA.json.gz          5  datePublished   \n",
      "2785   Book_2d-market.com_September2020_CPA.json.gz         11  priceCurrency   \n",
      "2786   Book_2d-market.com_September2020_CPA.json.gz          1    description   \n",
      "2787   Book_2d-market.com_September2020_CPA.json.gz          2     bookFormat   \n",
      "2788   Book_2d-market.com_September2020_CPA.json.gz          3  numberOfPages   \n",
      "2789   Book_2d-market.com_September2020_CPA.json.gz          4          genre   \n",
      "2790   Book_2d-market.com_September2020_CPA.json.gz          6       keywords   \n",
      "2791   Book_2d-market.com_September2020_CPA.json.gz          7         author   \n",
      "2792   Book_2d-market.com_September2020_CPA.json.gz          9   availability   \n",
      "2793   Book_2d-market.com_September2020_CPA.json.gz         10            url   \n",
      "54304  Book_2d-market.com_September2020_CPA.json.gz          0             -1   \n",
      "\n",
      "                                                    data  \n",
      "2783   17.49; 2.99; 3.99; 4.89; 4.99; 1.99; 2.79; 2.4...  \n",
      "2784   2018-06-01T00:00:00; 2014-12-31T00:00:00; 2016...  \n",
      "2785   EUR; EUR; EUR; EUR; EUR; EUR; EUR; EUR; EUR; E...  \n",
      "2786   30 years ago a rain of meteors showered the Ea...  \n",
      "2787   EBook; EBook; EBook; EBook; EBook; EBook; EBoo...  \n",
      "2788   38.0; 27.0; 17.0; 33.0; 27.0; 31.0; 23.0; 31.0...  \n",
      "2789   Action; Comedy; Erotic; Erotic; Erotic; Erotic...  \n",
      "2790   doujin, 同人誌; Ahegao, Anal, Big ass, Big breast...  \n",
      "2791   夢茶会; Muchakai; strelka; Sagejoh; Once Only; Me...  \n",
      "2792   http://schema.org/OnlineOnly; http://schema.or...  \n",
      "2793   https://2d-market.com/Comic/351; https://2d-ma...  \n",
      "54304  ちーちゃん開発日記カラー版; Chii-chan Development Diary Ful...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "filepath = \"./generate_prompt/test_semi_sotab-re-withunlabel.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "df['label'] = df['label'].map(id2type).fillna(-1)\n",
    "print(df[df['table_id'] == 'Book_2d-market.com_September2020_CPA.json.gz'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6480/6480 [00:30<00:00, 209.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "num_row = 3 # how many rows to include in the table prompt\n",
    "k = 5\n",
    "output_data = []\n",
    "\n",
    "for tqdm_idx, group_df in tqdm(df.groupby('table_id')):\n",
    "    group_df['column_id'] = group_df['column_id'].astype(int)\n",
    "    group_df.sort_values(by=['column_id'], inplace=True)\n",
    "    group_df.reset_index(drop=True, inplace=True)\n",
    "    group_df['data'] = group_df['data'].astype(str)\n",
    "\n",
    "    col_values = []\n",
    "    min_row = min([len(row['data'].split(\";\")) for row_idx, row in group_df.iterrows()])\n",
    "    min_row_len = min(min_row, num_row)\n",
    "    for column_index, col in group_df.iterrows():\n",
    "        col_values.append(col['data'].split(\";\")[:min_row_len])\n",
    "    # transpose the list\n",
    "    col_values = list(map(list, zip(*col_values)))\n",
    "\n",
    "    col_head = f\"[TAB] col: | \"\n",
    "    for column_index in range(1, len(group_df) + 1):\n",
    "        col_head += f\"Column {column_index} | \"\n",
    "\n",
    "    table_cell = \"\"\n",
    "    for row_idx in range(1, min_row_len + 1):\n",
    "        example_values = \" | \".join(col_values[row_idx-1])  # Extract first 5 \n",
    "        if row_idx == 1:\n",
    "            table_cell += f\"row {row_idx}: | {example_values} |\"\n",
    "        else:\n",
    "            table_cell += f\" [SEP] row {row_idx}: | {example_values} |\"\n",
    "\n",
    "    df_target = group_df[(group_df[\"label\"] != -1)]\n",
    "    for idx in range(len(df_target)):\n",
    "        column_index = df_target.iloc[idx][\"column_id\"]+1\n",
    "        label = df_target.iloc[idx][\"label\"]\n",
    "        \n",
    "        k = min(5, min_row)\n",
    "        entity_pairs = \", \".join(\n",
    "            f\"<({group_df.iloc[0]['data'].split(';')[i]}), ({df_target.iloc[idx]['data'].split(';')[i]})>\"\n",
    "            for i in range(k)\n",
    "        )\n",
    "\n",
    "        table_prompt = {}\n",
    "        table_prompt[\"table_id\"] = str(tqdm_idx)\n",
    "        table_prompt[\"instruction\"] = \"This is a relation extraction task. The goal for this task is to choose correct relation between two selected columns of the table from the given candidates.\"\n",
    "        table_prompt[\"question\"] = f\"The two selected column names are: <(Column 1),(Column {column_index})>. The entity pairs for these two columns are: {entity_pairs}, etc. The relation type candidates are: {all_label_names}. What are the correct relation type for the two selected columns (column name: <(Column 1),(Column {column_index})>. entity pairs: {entity_pairs}, etc)?\"\n",
    "        table_prompt[\"input\"] = col_head + table_cell\n",
    "        table_prompt[\"ground_truth\"] = [label]\n",
    "        table_prompt[\"output\"] = label\n",
    "        table_prompt[\"input_seg\"] = col_head + table_cell\n",
    "        output_data.append(table_prompt)\n",
    "    \n",
    "        \n",
    "output_file = './dataset/sotab-cpa.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(output_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gittables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identifier, name, description, title, value, status, latitude, longitude, comment, email, category, version, gender, address, location, price, amount, weight, startDate, productID, position, endDate, language, size, height, startTime, color, target, currency, author, birthDate, department, image, sku, parent, endTime, width, postalCode, elevation, model, area, keywords, device, creator, dateCreated, publisher, depth, releaseDate, permissionType, step, result, frequency, abstract\n"
     ]
    }
   ],
   "source": [
    "# read df\n",
    "import os\n",
    "import pandas as pd\n",
    "git = 'sp'\n",
    "cv = '0'\n",
    "\n",
    "filepath = f\"./generate_prompt/{git}_semi1_cv_{cv}.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "with open(\"./generate_prompt/type2id_gitsc.pkl\", \"rb\") as file: \n",
    "    type2id = pickle.load(file)\n",
    "id2type = {int(v): k for k, v in type2id.items()}\n",
    "types = list(type2id.keys())\n",
    "all_label_names = \", \".join(types)\n",
    "print(all_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/574 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 574/574 [00:01<00:00, 436.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "num_row = 5 # how many rows to include in the table prompt\n",
    "k = 10\n",
    "output_data = []\n",
    "\n",
    "for tqdm_idx, group_df in tqdm(df.groupby('table_id')):\n",
    "    group_df['col_idx'] = group_df['col_idx'].astype(int)\n",
    "    group_df['class_id'] = group_df['class_id'].astype(int)\n",
    "    group_df.sort_values(by=['col_idx'], inplace=True)\n",
    "    group_df.reset_index(drop=True, inplace=True)\n",
    "    group_df['data'] = group_df['data'].astype(str)\n",
    "\n",
    "    col_values = []\n",
    "    min_row_len = min([len(row['data'].split(\";\")) for row_idx, row in group_df.iterrows()])\n",
    "    min_row_len = min(min_row_len, num_row)\n",
    "    for column_index, col in group_df.iterrows():\n",
    "        col_values.append(col['data'].split(\";\")[:min_row_len])\n",
    "    # transpose the list\n",
    "    col_values = list(map(list, zip(*col_values)))\n",
    "\n",
    "    col_head = f\"[TAB] col: | \"\n",
    "    for column_index in range(1, len(group_df) + 1):\n",
    "        col_head += f\"Column {column_index} | \"\n",
    "\n",
    "    table_cell = \"\"\n",
    "    for row_idx in range(1, min_row_len + 1):\n",
    "        example_values = \" | \".join(col_values[row_idx-1])  # Extract first 5 \n",
    "        if row_idx == 1:\n",
    "            table_cell += f\"row {row_idx}: | {example_values} |\"\n",
    "        else:\n",
    "            table_cell += f\" [SEP] row {row_idx}: | {example_values} |\"\n",
    "\n",
    "    df_target = group_df[group_df[\"class_id\"] > -1]\n",
    "    for idx in range(len(df_target)):\n",
    "        column_index = df_target.iloc[idx][\"col_idx\"]+1\n",
    "        label = df_target.iloc[idx][\"class\"]\n",
    "        entities = \", \".join([f\"<{item.strip()}>\" for item in df_target.iloc[idx][\"data\"].split(\";\")[:k]])\n",
    "\n",
    "        table_prompt = {}\n",
    "        table_prompt[\"table_id\"] = str(tqdm_idx)\n",
    "        table_prompt[\"instruction\"] = \"This is a column type annotation task. The goal for this task is to choose the correct type for one selected column of the table from the given candidates.\"\n",
    "        table_prompt[\"input\"] = col_head + table_cell\n",
    "        table_prompt[\"question\"] = f\"The column 'Column {column_index}' contains following entities: {entities}, etc. The column type candidates are: {all_label_names}. What are the correct column type for this column (column name: Column {column_index}; entities: {entities}, etc)?\"\n",
    "        table_prompt[\"ground_truth\"] = [label.split(\":\")[1]]\n",
    "        table_prompt[\"entity\"] = [item.strip() for item in df_target.iloc[idx][\"data\"].split(\";\")[:k]]\n",
    "        table_prompt[\"output\"] = label.split(\":\")[1]\n",
    "        table_prompt[\"input_seg\"] = col_head + table_cell\n",
    "        output_data.append(table_prompt)\n",
    "    \n",
    "        \n",
    "output_file = f'./dataset/gittables-{git}-{cv}.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data analysis - sotab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "def match_closest_type(llm_output, valid_types):\n",
    "    match, score, _ = process.extractOne(llm_output, valid_types, scorer=fuzz.ratio)\n",
    "    return match   # Adjust threshold as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_micro_f1=0.1696, ts_macro_f1=0.1381\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "task = \"sotab-cpa\"\n",
    "pred_file = f\"./output/{task}.json\"\n",
    "with open(f\"./generate_prompt/type_vocab_{task}.txt\", \"r\") as file:\n",
    "    id2type = {int(line.split(\"\\t\")[0]): line.split(\"\\t\")[1].strip() for line in file}\n",
    "type2id = {v: k for k, v in id2type.items()}\n",
    "types = list(type2id.keys())\n",
    "\n",
    "with open(pred_file, \"r\") as f:\n",
    "    col_type = json.load(f)\n",
    "\n",
    "ground_truth_list = []\n",
    "pred_list = []\n",
    "for i in range(len(col_type)):\n",
    "    item = col_type[i]\n",
    "    if item[\"predict\"] == None:\n",
    "        continue\n",
    "\n",
    "    ground_truth = item[\"ground_truth\"]\n",
    "    # pred = item[\"predict\"].strip(\"</s>\").split(\",\")\n",
    "    pred = item[\"predict\"].split(\"</s>\")[0].split(\", \")\n",
    "    ground_truth_list.append(ground_truth)\n",
    "    pred_list.append(pred)\n",
    "ts_true_list = [type2id[l[0]] for l in ground_truth_list]\n",
    "\n",
    "ts_pred_list = []\n",
    "for p in pred_list:\n",
    "    pred_i = ', '.join(p)\n",
    "    if pred_i in types:\n",
    "        ts_pred_list.append(type2id[pred_i])\n",
    "    else:\n",
    "        ts_pred_list.append(type2id[match_closest_type(pred_i, types)])\n",
    "\n",
    "ts_micro_f1 = f1_score(ts_true_list,\n",
    "                    ts_pred_list,\n",
    "                    average=\"micro\")\n",
    "ts_macro_f1 = f1_score(ts_true_list,\n",
    "                    ts_pred_list,\n",
    "                     average=\"macro\")\n",
    "\n",
    "micro_f1_scores.append(ts_micro_f1)\n",
    "macro_f1_scores.append(ts_macro_f1)\n",
    "print(f\"ts_micro_f1={ts_micro_f1:.4f}, ts_macro_f1={ts_macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data analysis - GitTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "def match_closest_type(llm_output, valid_types):\n",
    "    match, score, _ = process.extractOne(llm_output, valid_types, scorer=fuzz.ratio)\n",
    "    return match   # Adjust threshold as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0: ts_micro_f1=0.1266, ts_macro_f1=0.1054\n",
      "CV 1: ts_micro_f1=0.1362, ts_macro_f1=0.1277\n",
      "CV 2: ts_micro_f1=0.1234, ts_macro_f1=0.1013\n",
      "CV 3: ts_micro_f1=0.1274, ts_macro_f1=0.1351\n",
      "CV 4: ts_micro_f1=0.1111, ts_macro_f1=0.1005\n",
      "Average ts_micro_f1 (Percentage)=12.49%, Standard Deviation ts_micro_f1 (Percentage)=0.9088%\n",
      "Average ts_macro_f1 (Percentage)=11.40%, Standard Deviation ts_macro_f1 (Percentage)=1.6207%\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for cv in [\"0\", \"1\", \"2\", \"3\", \"4\"]:\n",
    "    task = f\"gittables-sp-{cv}\"\n",
    "    pred_file = f\"./output/{task}.json\"\n",
    "\n",
    "    with open(\"./generate_prompt/type2id_gitsc.pkl\", \"rb\") as file: \n",
    "        type2id = pickle.load(file)\n",
    "    id2type = {int(v): k for k, v in type2id.items()}\n",
    "    types = list(type2id.keys())\n",
    "\n",
    "    with open(pred_file, \"r\") as f:\n",
    "        col_type = json.load(f)\n",
    "\n",
    "    ground_truth_list = []\n",
    "    pred_list = []\n",
    "    for i in range(len(col_type)):\n",
    "        item = col_type[i]\n",
    "        if item[\"predict\"] is None:\n",
    "            continue\n",
    "\n",
    "        ground_truth = item[\"ground_truth\"]\n",
    "        pred = item[\"predict\"].split(\"</s>\")[0].split(\", \")\n",
    "        ground_truth_list.append(ground_truth)\n",
    "        pred_list.append(pred)\n",
    "\n",
    "    ts_true_list = [type2id[l[0]] for l in ground_truth_list]\n",
    "\n",
    "    ts_pred_list = []\n",
    "    for p in pred_list:\n",
    "        pred_i = ', '.join(p)\n",
    "        if pred_i in types:\n",
    "            ts_pred_list.append(type2id[pred_i])\n",
    "        else:\n",
    "            ts_pred_list.append(type2id[match_closest_type(pred_i, types)])\n",
    "\n",
    "    ts_micro_f1 = f1_score(ts_true_list, ts_pred_list, average=\"micro\")\n",
    "    ts_macro_f1 = f1_score(ts_true_list, ts_pred_list, average=\"macro\")\n",
    "    \n",
    "    micro_f1_scores.append(ts_micro_f1)\n",
    "    macro_f1_scores.append(ts_macro_f1)\n",
    "    print(f\"CV {cv}: ts_micro_f1={ts_micro_f1:.4f}, ts_macro_f1={ts_macro_f1:.4f}\")\n",
    "\n",
    "# 计算均值和标准差\n",
    "avg_micro_f1 = statistics.mean(micro_f1_scores)\n",
    "std_micro_f1 = statistics.stdev(micro_f1_scores)\n",
    "\n",
    "avg_macro_f1 = statistics.mean(macro_f1_scores)\n",
    "std_macro_f1 = statistics.stdev(macro_f1_scores)\n",
    "\n",
    "# 转换为百分制并保留四位小数\n",
    "avg_micro_f1_percentage = avg_micro_f1 * 100\n",
    "std_micro_f1_percentage = std_micro_f1 * 100\n",
    "\n",
    "avg_macro_f1_percentage = avg_macro_f1 * 100\n",
    "std_macro_f1_percentage = std_macro_f1 * 100\n",
    "\n",
    "# 输出结果\n",
    "print(f\"Average ts_micro_f1 (Percentage)={avg_micro_f1_percentage:.2f}%, Standard Deviation ts_micro_f1 (Percentage)={std_micro_f1_percentage:.4f}%\")\n",
    "print(f\"Average ts_macro_f1 (Percentage)={avg_macro_f1_percentage:.2f}%, Standard Deviation ts_macro_f1 (Percentage)={std_macro_f1_percentage:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data analysis - TURL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import process, fuzz\n",
    "import json\n",
    "import pickle\n",
    "from sklearn.metrics import multilabel_confusion_matrix, f1_score\n",
    "\n",
    "def match_closest_type(llm_output, valid_types):\n",
    "    match, score, _ = process.extractOne(llm_output, valid_types, scorer=fuzz.ratio)\n",
    "    return match   # Adjust threshold as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baseball.baseball_player.batting_stats-baseball.batting_statistics.team', 'book.author.works_written', 'time.event.locations', 'film.film.genre', 'government.politician.party-government.political_party_tenure.party', 'people.person.place_of_birth', 'film.film.release_date_s-film.film_regional_release_date.film_release_region', 'american_football.football_player.position_s', 'cvg.computer_videogame.publisher', 'people.person.spouse_s-people.marriage.spouse', 'film.actor.film-film.performance.character', 'sports.sports_team.sport', 'base.biblioness.bibs_location.country', 'sports.sports_facility.home_venue_for-sports.team_venue_relationship.team', 'award.award_nominated_work.award_nominations-award.award_nomination.ceremony', 'film.film.directed_by', 'base.culturalevent.event.entity_involved', 'music.album.artist', 'sports.sports_team.location', 'sports.sports_position.players-sports.sports_team_roster.position', 'sports.pro_athlete.teams-sports.sports_team_roster.position', 'soccer.football_player.statistics-soccer.football_player_stats.team', 'basketball.basketball_player.player_statistics-basketball.basketball_player_stats.team', 'location.hud_county_place.county', 'sports.sports_team.league-sports.sports_league_participation.league', 'aviation.airline.hubs', 'organization.organization.headquarters-location.mailing_address.state_province_region', 'sports.sports_championship_event.champion', 'location.location.nearby_airports', 'people.person.parents', 'film.film.distributors-film.film_film_distributor_relationship.distributor', 'sports.sports_facility.teams', 'film.film.written_by', 'base.wikipedia_infobox.video_game.developer', 'film.film.produced_by', 'award.award_winning_work.awards_won-award.award_honor.award_winner', 'baseball.baseball_team.league', 'award.award_nominated_work.award_nominations-award.award_nomination.award_nominee', 'education.educational_institution.athletics_brand', 'olympics.olympic_participating_country.athletes-olympics.olympic_athlete_affiliation.athlete', 'location.administrative_division.capital-location.administrative_division_capital_relationship.capital', 'aviation.airport.serves', 'award.award_ceremony.nominees-award.award_nomination.award_nominee', 'location.location.containedby', 'tv.tv_actor.starring_roles-tv.regular_tv_appearance.character', 'location.country.official_language', 'tv.tv_character.appeared_in_tv_program-tv.regular_tv_appearance.actor', 'base.schemastaging.athlete_extra.salary-base.schemastaging.athlete_salary.team', 'sports.sports_award_winner.awards-sports.sports_award.team', 'olympics.olympic_event_competition.medalists-olympics.olympic_medal_honor.medalist', 'sports.sports_team.league-sports.sports_league_participation.from', 'location.location.street_address-location.mailing_address.citytown', 'soccer.football_player.position_s', 'location.location.contains', 'music.composition.composer', 'ice_hockey.hockey_player.hockey_position', 'government.political_party.politicians_in_this_party-government.political_party_tenure.politician', 'base.wikipedia_infobox.video_game.platforms', 'government.governmental_jurisdiction.governing_officials-government.government_position_held.office_holder', 'government.politician.government_positions_held-government.government_position_held.district_represented', 'film.film.language', 'military.military_person.service-military.military_service.military_force', 'award.award_category.winners-award.award_honor.award_winner', 'tv.tv_network.programs-tv.tv_network_duration.program', 'soccer.football_team.player_statistics-soccer.football_player_stats.player', 'film.director.film', 'organization.organization.headquarters-location.mailing_address.country', 'sports.sports_league.teams-sports.sports_league_participation.team', 'award.award_winner.awards_won-award.award_honor.honored_for', 'location.country.capital', 'tv.tv_program.country_of_origin', 'film.film_character.portrayed_in_films-film.performance.actor', 'location.country.languages_spoken', 'film.film.country', 'base.schemastaging.organization_extra.phone_number-base.schemastaging.phone_sandbox.service_location', 'time.event.instance_of_recurring_event', 'government.political_district.representatives-government.government_position_held.office_holder', 'cvg.computer_videogame.developer', 'military.military_combatant.military_conflicts-military.military_combatant_group.combatants', 'olympics.olympic_athlete.medals_won-olympics.olympic_medal_honor.country', 'base.aareas.schema.administrative_area.administrative_parent', 'music.composer.compositions', 'award.award_nominee.award_nominations-award.award_nomination.nominated_for', 'film.film.starring-film.performance.character', 'olympics.olympic_athlete.medals_won-olympics.olympic_medal_honor.event', 'olympics.olympic_participating_country.medals_won-olympics.olympic_medal_honor.medalist', 'people.person.places_lived-people.place_lived.location', 'sports.pro_athlete.teams-sports.sports_team_roster.team', 'film.film.music', 'people.person.nationality', 'tv.tv_program.original_network-tv.tv_network_duration.network', 'award.award_ceremony.nominees-award.award_nomination.nominated_for', 'film.actor.film-film.performance.film', 'people.deceased_person.place_of_death', 'sports.sports_team.roster-sports.sports_team_roster.player', 'sports.sports_position.players-sports.sports_team_roster.player', 'book.written_work.author', 'location.administrative_division.country', 'olympics.olympic_athlete.country-olympics.olympic_athlete_affiliation.country', 'sports.sports_league_season.league', 'sports.sports_team.venue-sports.team_venue_relationship.venue', 'meteorology.tropical_cyclone.tropical_cyclone_season', 'award.award_category.category_of', 'film.film.story_by', 'broadcast.broadcast.area_served', 'film.film.production_companies', 'award.award_category.nominees-award.award_nomination.ceremony', 'music.artist.album', 'award.award_category.nominees-award.award_nomination.award_nominee', 'film.film.starring-film.performance.actor', 'organization.organization.headquarters-location.mailing_address.citytown', 'tv.tv_actor.starring_roles-tv.regular_tv_appearance.series', 'baseball.baseball_player.position_s', 'award.award_category.winners-award.award_honor.ceremony', 'people.person.education-education.education.institution', 'tv.tv_program.regular_cast-tv.regular_tv_appearance.actor', 'award.award_ceremony.awards_presented-award.award_honor.award_winner', 'american_football.football_player.games-american_football.player_game_statistics.team', 'location.capital_of_administrative_division.capital_of-location.administrative_division_capital_relationship.administrative_division', 'sports.sports_team.arena_stadium', 'award.award_ceremony.awards_presented-award.award_honor.honored_for']\n"
     ]
    }
   ],
   "source": [
    "task = f\"turl-cpa\"\n",
    "pred_file = f\"./output/{task}.json\"\n",
    "with open(f\"./generate_prompt/type_vocab_{task}.txt\", \"r\") as file:\n",
    "    id2type = {int(line.split(\"\\t\")[0]): line.split(\"\\t\")[1].strip() for line in file}\n",
    "type2id = {v: k for k, v in id2type.items()}\n",
    "types = list(type2id.keys())\n",
    "\n",
    "with open(pred_file, \"r\") as f:\n",
    "    col_type = json.load(f)\n",
    "\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def f1_score_multilabel(true_list, pred_list):\n",
    "    conf_mat = multilabel_confusion_matrix(np.array(true_list),\n",
    "                                           np.array(pred_list))\n",
    "    agg_conf_mat = conf_mat.sum(axis=0)\n",
    "    # Note: Pos F1\n",
    "    # [[TN FP], [FN, TP]] if we consider 1 as the positive class\n",
    "    p = agg_conf_mat[1, 1] / agg_conf_mat[1, :].sum()\n",
    "    r = agg_conf_mat[1, 1] / agg_conf_mat[:, 1].sum()\n",
    "    \n",
    "    micro_f1 = 2 * p * r / (p  + r) if (p + r) > 0 else 0.\n",
    "    class_p = conf_mat[:, 1, 1] /  conf_mat[:, 1, :].sum(axis=1)\n",
    "    class_r = conf_mat[:, 1, 1] /  conf_mat[:, :, 1].sum(axis=1)\n",
    "    class_f1 = np.divide(2 * (class_p * class_r), class_p + class_r,\n",
    "                         out=np.zeros_like(class_p), where=(class_p + class_r) != 0)\n",
    "    class_f1 = np.nan_to_num(class_f1)\n",
    "    macro_f1 = class_f1.mean()\n",
    "    return (micro_f1, macro_f1, class_f1, conf_mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_f1=0.9262, macro_f1=0.8479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_389775/1523030160.py:12: RuntimeWarning: invalid value encountered in divide\n",
      "  class_p = conf_mat[:, 1, 1] /  conf_mat[:, 1, :].sum(axis=1)\n",
      "/tmp/ipykernel_389775/1523030160.py:13: RuntimeWarning: invalid value encountered in divide\n",
      "  class_r = conf_mat[:, 1, 1] /  conf_mat[:, :, 1].sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "ground_truth_list = []\n",
    "pred_list = []\n",
    "for i in range(len(col_type)):\n",
    "    item = col_type[i]\n",
    "    if item[\"predict\"] == None:\n",
    "        continue\n",
    "\n",
    "    ground_truth = item[\"ground_truth\"]\n",
    "    pred = item[\"predict\"].split(\"</s>\")[0].split(\", \")\n",
    "\n",
    "    true_vector = [0] * 121\n",
    "    pred_vector = [0] * 121\n",
    "\n",
    "    for label in ground_truth:\n",
    "        true_vector[type2id[label]] = 1  # 假设 type2id[label] 返回类别索引\n",
    "\n",
    "    for label in pred:\n",
    "        if label in types:\n",
    "            pred_vector[type2id[label]] = 1\n",
    "        else:\n",
    "            pred_vector[type2id[match_closest_type(label, types)]] = 1\n",
    "\n",
    "    ground_truth_list.append(true_vector)\n",
    "    pred_list.append(pred_vector)\n",
    "                                                  \n",
    "\n",
    "ts_micro_f1, ts_macro_f1, ts_class_f1, ts_conf_mat = f1_score_multilabel(ground_truth_list, pred_list)\n",
    "print(f\"micro_f1={ts_micro_f1:.4f}, macro_f1={ts_macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "observatory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
